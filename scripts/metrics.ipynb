{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b90ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebacf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac61a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names= []\n",
    "feature_types = [ \"boroujeni_et_al\", \"chen_cui\", \"marras_et_al\", \"lalle_conati\"]\n",
    "\n",
    "for feature_type in feature_types:\n",
    "    filepath = '../ex-epfl-mooc/scripts/feature_names/' + feature_type + '.csv'\n",
    "    feature_type_name = pd.read_csv(filepath,header=None)\n",
    "    feature_type_name = feature_type_name.values.reshape(-1)\n",
    "    feature_names.append(feature_type_name)\n",
    "    print(feature_type_name.shape)\n",
    "    \n",
    "feature_names = np.concatenate(feature_names)\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f52dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create normalized LIME results\n",
    "folder = \"../ex-epfl-mooc/uniform_eq_results\"\n",
    "courses = ['dsp_001', 'dsp_002', 'geomatique_003', 'villesafricaines_001', 'microcontroleurs_003']\n",
    "lime_normalized = []\n",
    "for course in courses:\n",
    "    extract_file = folder + \"/LIME/\" + course + \"/dataframes/all_important_features.csv\"\n",
    "    df = pd.read_csv(extract_file, header=0)\n",
    "    df.drop(['exp number', 'real value'], axis = 1, inplace = True) \n",
    "    if normalization:\n",
    "        normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "        normalized_df = normalized_df.div(normalized_df.sum(axis=1), axis=0)\n",
    "    else:\n",
    "        normalized_df = df\n",
    "    df = pd.read_csv(extract_file, header=0)\n",
    "    normalized_df['exp number'] = df['exp number']\n",
    "    normalized_df['real value'] = df['real value']\n",
    "    normalized_df.to_csv('normalized_LIME_' + course +\".csv\")\n",
    "    lime_normalized.append((course,normalized_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized Permutation SHAP results\n",
    "folder = \"../ex-epfl-mooc/uniform_eq_results\"\n",
    "courses = ['dsp_001', 'dsp_002', 'geomatique_003', 'villesafricaines_001', 'microcontroleurs_003']\n",
    "perm_shap_normalized = []\n",
    "for course in courses:\n",
    "    extract_file = folder + \"/SHAP/Permutation/\" + course + \".csv\"\n",
    "    df = pd.read_csv(extract_file, header=0)\n",
    "    df.drop(['Unnamed: 0', 'exp_num'], axis = 1, inplace = True) \n",
    "    if normalization:\n",
    "        normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "        normalized_df = normalized_df.div(normalized_df.sum(axis=1), axis=0)\n",
    "    else:\n",
    "        normalized_df = df\n",
    "    df = pd.read_csv(extract_file, header=0)\n",
    "    normalized_df['exp number'] = df['exp_num']\n",
    "    normalized_df.to_csv('normalized_SHAP_' + course +\".csv\")\n",
    "    perm_shap_normalized.append((course,normalized_df))\n",
    "# #     df.drop(['exp number', 'real value'], axis = 1, inplace = True) \n",
    "#     normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "#     normalized_df.to_csv('normalized_SHAP_' + course +\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counterfactuals\n",
    "folder = \"../ex-epfl-mooc/uniform_eq_results\"\n",
    "courses = ['dsp_001', 'dsp_002', 'geomatique_003', 'villesafricaines_001', 'microcontroleurs_003']\n",
    "counter_normalized = []\n",
    "for course in courses:\n",
    "    shap_df = pd.read_csv('normalized_SHAP_' + course +\".csv\", header=0)\n",
    "    cf_list = []\n",
    "    for i in shap_df['exp number']:\n",
    "        extract_file = folder + \"/Counterfactuals/\" + course + \"/feature_importances_\" + str(i) + \".csv\"\n",
    "        try:\n",
    "            instance = pd.read_csv(extract_file)\n",
    "            cf_list.append(instance)\n",
    "        except:\n",
    "            cf_list.append(pd.DataFrame())\n",
    "    if len(cf_list) > 1:\n",
    "        df = pd.concat(cf_list, axis=0)\n",
    "        if normalization:\n",
    "            normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "            normalized_df = normalized_df.div(normalized_df.sum(axis=1), axis=0)\n",
    "        else:\n",
    "            normalized_df = df\n",
    "        normalized_df['exp number'] = shap_df['exp number']\n",
    "        normalized_df.to_csv('normalized_Counterfactuals_' + course +\".csv\")\n",
    "        counter_normalized.append((course, normalized_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368528d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CEM\n",
    "folder = \"../ex-epfl-mooc/uniform_eq_results\"\n",
    "courses = ['dsp_001', 'dsp_002', 'geomatique_003', 'villesafricaines_001', 'microcontroleurs_003']\n",
    "cem_normalized = []\n",
    "for course in courses:\n",
    "    extract_file = folder + \"/CEM/\" + course + \"/importances.csv\"\n",
    "    df = pd.read_csv(extract_file, header=0)\n",
    "    df.drop(['Unnamed: 0', 'exp_num'], axis = 1, inplace = True) \n",
    "    if normalization:\n",
    "        normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "        normalized_df = normalized_df.div(normalized_df.sum(axis=1), axis=0)\n",
    "    else:\n",
    "        normalized_df = df\n",
    "    df = pd.read_csv(extract_file, header=0)\n",
    "    normalized_df['exp number'] = df['exp_num']\n",
    "    normalized_df.to_csv('normalized_CEM_' + course +\".csv\")\n",
    "    cem_normalized.append((course,normalized_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8f5375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized Kernel SHAP results\n",
    "folder = \"../ex-epfl-mooc/uniform_eq_results\"\n",
    "courses = ['dsp_001', 'dsp_002', 'geomatique_003', 'villesafricaines_001', 'microcontroleurs_003']\n",
    "kernel_shap_normalized = []\n",
    "for course in courses:\n",
    "    extract_file = folder + \"/SHAP/Kernel/\" + course + \".csv\"\n",
    "    try:\n",
    "         df = pd.read_csv(extract_file, header=0)\n",
    "    except:\n",
    "        kernel_shap_normalized.append((course, pd.DataFrame()))\n",
    "        continue\n",
    "    df.drop(['Unnamed: 0', 'exp_num'], axis = 1, inplace = True) \n",
    "    if normalization:\n",
    "        normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "        normalized_df = normalized_df.div(normalized_df.sum(axis=1), axis=0)\n",
    "    else:\n",
    "        normalized_df = df\n",
    "    df = pd.read_csv(extract_file, header=0)\n",
    "    normalized_df['exp number'] = df['exp_num']\n",
    "    normalized_df.to_csv('normalized_SHAP_' + course +\".csv\")\n",
    "    kernel_shap_normalized.append((course,normalized_df))\n",
    "# #     df.drop(['exp number', 'real value'], axis = 1, inplace = True) \n",
    "#     normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "#     normalized_df.to_csv('normalized_SHAP_' + course +\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c22aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_names = ['dsp_001', 'dsp_002', 'geomatique_003', 'villesafricaines_003', 'microcontroleurs_003']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807689a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cem_vals, perm_shap_vals, lime_vals, dice_vals, kernel_shap_vals = [], [], [], [], []\n",
    "\n",
    "def fit_pca(pca, df):\n",
    "    if len(df) < 1:\n",
    "        return []\n",
    "    pca.fit(df.select_dtypes(['number']).select_dtypes(['number']).fillna(0))\n",
    "    return pca.singular_values_\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "# for i,course in enumerate(courses):\n",
    "#     cem_vals.append(fit_pca(pca, cem_normalized[i][1]))\n",
    "#     kernel_shap_vals.append(fit_pca(pca, kernel_shap_normalized[i][1]))\n",
    "#     perm_shap_vals.append(fit_pca(pca, perm_shap_normalized[i][1]))\n",
    "#     lime_vals.append(fit_pca(pca, lime_normalized[i][1]))\n",
    "#     dice_vals.append(fit_pca(pca, counter_normalized[i][1]))\n",
    "\n",
    "for i,course in enumerate(courses):\n",
    "    cem_vals.append(fit_pca(pca, all_points[(course, 'CEM')]))\n",
    "    kernel_shap_vals.append(fit_pca(pca, all_points[(course, 'KernelSHAP')]))\n",
    "    perm_shap_vals.append(fit_pca(pca, all_points[(course, 'PermSHAP')]))\n",
    "    lime_vals.append(fit_pca(pca, all_points[(course, 'LIME')]))\n",
    "    dice_vals.append(fit_pca(pca, all_points[(course, 'DiCE')]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = [cem_vals, kernel_shap_vals, perm_shap_vals, lime_vals, dice_vals]\n",
    "total_list = []\n",
    "[total_list.extend(l) for l in vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3289dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = pd.DataFrame(total_list, columns=['pca1', 'pca2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef132ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03db2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_names = []\n",
    "pca_dim = 5\n",
    "# vals = [cem_vals, kernel_shap_vals, perm_shap_vals, lime_vals, dice_vals]\n",
    "courses = ['dsp_001', 'dsp_002', 'geomatique_003', 'villesafricaines_001', 'microcontroleurs_003']\n",
    "names = [['cem'] * pca_dim + ['kernel_shap']*pca_dim, ['perm_shap'] * pca_dim, ['lime'] * pca_dim, ['dice'] * pca_dim]\n",
    "[total_names.extend(l) for l in names]\n",
    "vals['method'] = total_names\n",
    "vals['course'] = courses * 5\n",
    "vals['method_pretty'] = np.concatenate([['CEM'] * pca_dim + ['KernelSHAP']*pca_dim, ['PermSHAP'] * pca_dim, ['LIME'] * pca_dim, ['DiCE'] * pca_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cdbf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2451c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols.append(['AvgReplayedWeeklyProp',\n",
    " 'AvgTimeSessions',\n",
    " 'CompetencyAlignment',\n",
    " 'CompetencyAnticipation',\n",
    " 'CompetencyStrength',\n",
    " 'ContentAlignment',\n",
    " 'ContentAnticipation',\n",
    " 'DelayLecture',\n",
    " 'RatioClicksWeekendDay',\n",
    " 'RegPeakTimeDayHour',\n",
    " 'RegPeriodicityM1',\n",
    " 'StdTimeSessions',\n",
    " 'StudentShape',\n",
    " 'StudentSpeed',\n",
    " 'TotalTimeProblem',\n",
    " 'TotalTimeVideo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = set(np.concatenate(all_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0405444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For LIME df is explanations for each instance stacked together features not present in one instance have nan as their value\n",
    "course_index = 0\n",
    "num_weeks = 10\n",
    "course = course_names[course_index]\n",
    "exp_sets = [\n",
    "    ('LIME', lime_normalized),\n",
    "    ('KernelSHAP', kernel_shap_normalized),\n",
    "    ('PermSHAP', perm_shap_normalized),\n",
    "    ('CEM', cem_normalized),\n",
    "    ('DiCE', counter_normalized)\n",
    "]\n",
    "for method, overall_df in exp_sets:\n",
    "    df = overall_df[course_index][1]\n",
    "    if 'exp number' in df.columns:\n",
    "        df = df.drop(['exp number'], axis=1)\n",
    "             \n",
    "    if 'exp_num' in df.columns:\n",
    "        df = df.drop(['exp_num'], axis=1)\n",
    "             \n",
    "    if 'real value' in df.columns:\n",
    "        df = df.drop(['real value'], axis=1)\n",
    "    \n",
    "    df = abs(df)\n",
    "    if \"LIME\" not in method:\n",
    "        sorted_values = np.argsort(abs(df), axis = 1) #.drop('exp_num', axis=1)\n",
    "    #     df = df.drop('exp_num', axis=1)\n",
    "    #     print(sorted_values.shape, df.shape)\n",
    "    #     df = sorted_values.iloc[:, df.shape[1]-10:]\n",
    "        ind = sorted_shap_values.iloc[:,:df.shape[1]-10]\n",
    "    #     print(ind)\n",
    "    #     print(df.shape[0])\n",
    "        for i in np.arange(df.shape[0]):\n",
    "            df.iloc[i,ind.iloc[i,:]]=np.nan #features outside top ten is replaced with nans\n",
    "        df = df.iloc[:,~((df.isnull().sum(axis=0)==df.shape[0]).values)]#eliminating features with all nans\n",
    "\n",
    "    ai = np.argsort(df.values) #nans will come after numbers\n",
    "    for j,c in enumerate(list(ai[:,:10])): # changing feature importance to scores\n",
    "        df.iloc[j,c] = np.arange(1,11)\n",
    "    top_features = df.columns\n",
    "    top_features_type = np.array([s[0:s.find('_InWeek')].split(' ')[-1] for s in top_features])\n",
    "    top_features_week = np.array([s[s.find('_InWeek')+7::].split(' ')[0] for s in top_features])\n",
    "    count = np.nansum(df.values,axis=0)#summing without the nans-----total score\n",
    "    top_features_type_unique = list({ k for k in top_features_type })\n",
    "    top_features_week_unique = list({ k for k in top_features_week })\n",
    "    ######\n",
    "    zero_data = np.zeros(shape=(num_weeks,len(top_features_type_unique)))\n",
    "    d = pd.DataFrame(zero_data, columns=top_features_type_unique)\n",
    "    for i,f in enumerate(top_features_type):\n",
    "        d[f][int(top_features_week[i])-1]+=count[i]\n",
    "    d = d/(df.shape[0]*10)\n",
    "    d = d.iloc[:,((d>=0.333333).sum(axis=0)>0).values]#cutting criteria\n",
    "\n",
    "    col_rename = {}\n",
    "    for col in df.columns:\n",
    "        col_rename[col.split('_InWeek')[0]] = mapping(col.split('_InWeek')[0])\n",
    "    d = d.rename(columns=col_rename)\n",
    "    \n",
    "    for col in all_cols:\n",
    "        if col not in d.columns:\n",
    "            new_col = np.empty((len(d),1))\n",
    "            new_col.fill(0)\n",
    "            d.insert(0, col, new_col)\n",
    "    \n",
    "    d = d[all_cols]\n",
    "    ######\n",
    "    fig, ax = plt.subplots(figsize=(10, 4),facecolor='white')\n",
    "    cmap = sns.light_palette(\"purple\",n_colors=20)\n",
    "    d[d.values<0.01]=np.nan\n",
    "    annot = np.vectorize(lambda x: '' if x<0.01 else str(round(x,2)))(d.T.to_numpy())\n",
    "    heatmap_font = 12\n",
    "    g = sns.heatmap(d.values.T, annot=annot, fmt=\"\", vmin=0, vmax=1, annot_kws={\"fontsize\":heatmap_font, \"weight\":'bold'},cmap=cmap, square=True)\n",
    "    g.set_xticklabels(np.arange(1,num_weeks+1),rotation=0)\n",
    "    g.set_yticklabels(d.columns.values,rotation=0)\n",
    "#     border = 0.5\n",
    "#     g.axhline(y = 0, color='k',linewidth = border)\n",
    "#     g.axhline(y = d.shape[1], color = 'k',\n",
    "#                     linewidth = border)\n",
    "#     g.axvline(x = 0, color = 'k',\n",
    "#                     linewidth = border)\n",
    "#     g.axvline(x = d.shape[0], \n",
    "#                     color = 'k', linewidth = border)\n",
    "    g.set_title( 'Important features heatmap '+ course + \" \" +  method, fontsize=15)\n",
    "    plt.savefig('plots/important_features_' + course + \"_\" + method + \".svg\",  bbox_inches = 'tight', facecolor=fig.get_facecolor())\n",
    "    # pyplot.savefig(“./LIME_results/” + group + r”/pyplots/heatmap.png”, bbox_inches = ‘tight’, facecolor=fig.get_facecolor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b52dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f9a7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from matplotlib import rc\n",
    "import matplotlib\n",
    "matplotlib.rc_file_defaults()\n",
    "\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern Bold']})\n",
    "rc('text', usetex=True)\n",
    "rc('text.latex', preamble=r'\\usepackage{amsmath}\\usepackage{amssymb}')\n",
    "\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "course_names = ['DSP 1', 'DSP 2', 'Geomatique', 'Villes Africaines', 'Microcontroleurs']\n",
    "\n",
    "for course_index in np.arange(5):\n",
    "    course = courses[course_index]\n",
    "#     n = ['   CEM', '  DICE', '   KernelSHAP', '   LIME', '  PermSHAP']\n",
    "    vals_df =vals[vals['course'] == course]\n",
    "    x = vals_df['pca1']\n",
    "    y = vals_df['pca2']\n",
    "    t = np.arange(5)\n",
    "    plt.figure()\n",
    "    plt.scatter(x, y, c=t, cmap=\"rainbow\", marker='*')\n",
    "    for i, txt in enumerate(vals_df['method_pretty']):\n",
    "        if 'Kernel' in txt:\n",
    "            plt.annotate(txt, (x[i*5+course_index]-0.4, y[i*5+course_index]+0.03))\n",
    "        else: \n",
    "            if 'LIME' in txt:\n",
    "                plt.annotate(txt, (x[i*5+course_index]+0.05, y[i*5+course_index] - 0.07))\n",
    "            else:\n",
    "                plt.annotate(txt, (x[i*5+course_index]+0.05, y[i*5+course_index]))\n",
    "    plt.title('PCA Comparison of Feature Importances - '+ course_names[course_index] + '\\n') \n",
    "    plt.xlim(0, 2.6)\n",
    "    plt.ylim(0, 1.25)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.savefig(\"plots/\" + course + \"_PCA_analysis.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d540d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "course = courses[course_index]\n",
    "vals_df =vals.groupby('method_pretty').mean()\n",
    "x=vals_df['pca1']\n",
    "y=vals_df['pca2']\n",
    "t=np.arange(5)\n",
    "plt.scatter(x, y, c=t, cmap=cm.plasma)\n",
    "for i, txt in enumerate(vals_df.index):\n",
    "    if x[i] > 1.2:\n",
    "            plt.annotate(txt, (x[i]-0.25, y[i]))\n",
    "    else:\n",
    "        if 'Kernel' in txt:\n",
    "            plt.annotate(txt, (x[i]-0.01, y[i]+0.05))\n",
    "        else: \n",
    "            plt.annotate(txt, (x[i]+0.05, y[i]))\n",
    "plt.xlim(0, 2.6)\n",
    "plt.ylim(0, 1.25)\n",
    "plt.title('Averaged PCA Analysis of Explainability Method Feature Importances \\n')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.savefig('plots/averaged_pca_analysis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3bb0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = pd.DataFrame(pca_vals, columns=['pca1', 'pca2'])\n",
    "vals_methods = vals # [vals['method'].isin(['perm_shap', 'lime']) ]\n",
    "markers = ['*', '.', '+', '1', 'd']\n",
    "for j,course in enumerate(courses):\n",
    "    subset = vals_methods[vals_methods['course'] == course]\n",
    "    x=np.array(subset['pca1'])\n",
    "    y=np.array(subset['pca2'])\n",
    "    t=np.arange(5)\n",
    "    plt.scatter(x, y, c=t, cmap='rainbow', marker=markers[j])\n",
    "    n = ['d1', 'd2', 'geo', 'va', 'm']\n",
    "    for i, txt in enumerate(vals_methods['method_pretty']):\n",
    "        if j==1 and i%5 == 0:\n",
    "            if 'LIME' in txt:\n",
    "                plt.annotate(txt, (x[3]-0.4, y[3]+0.07))\n",
    "            else:\n",
    "                if 'CEM' in txt:\n",
    "                    plt.annotate(txt, (x[0]-0.1, y[0]+0.08))\n",
    "                else:\n",
    "                    if 'DiCE' in txt:\n",
    "                        plt.annotate(txt, (x[4]-0.25, y[4]-0.08))\n",
    "                    if 'Kernel' in txt:\n",
    "                        plt.annotate(txt, (x[1]-0.1, y[1]+0.08))\n",
    "                    if 'Perm' in txt:\n",
    "                        plt.annotate(txt, (x[2]+0.18, y[2]-0.05))\n",
    "    plt.title('All Courses - PCA Analysis of Explainability Method Feature Importances\\n')\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.savefig('plots/all_courses_pca.svg')\n",
    "    # ax.legend([0, 1, 2, 3, 4],['cem', 'kernel_shap', 'perm_shap', 'lime', 'dice'], loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719a109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f27f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b0e9b5c",
   "metadata": {},
   "source": [
    "### Quantitative Metrics: Frobenius Norm + Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b494fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(exp_sets).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b165f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_set = counter_normalized\n",
    "# method = 'DICE'\n",
    "# for i in range(len(courses)):\n",
    "df = experiment_set[0][1]\n",
    "for method, experiment_set in exp_sets:\n",
    "    print(method)\n",
    "    if len(df) > 1:\n",
    "        plt.figure()\n",
    "        if method == 'DICE':\n",
    "            df = df.drop('exp number',axis=1).mean() #.drop('exp_num',axis=1).drop('Unnamed: 0',axis=1)\n",
    "        else:\n",
    "            df = df.drop('exp number',axis=1).mean()\n",
    "        df = pd.DataFrame(df)\n",
    "        print(df.index)\n",
    "        df['week'] = [int(i.split('_InWeek')[-1]) for i in df.index]\n",
    "        df['feature_name'] = [i.split('_InWeek')[0] for i in df.index]\n",
    "        heatmap_df = df.pivot( \"feature_name\",\"week\", 0)\n",
    "        ax = sns.heatmap(heatmap_df)\n",
    "        ax.set_title(courses[0] + \"- \" + method)\n",
    "        plt.savefig(\"Heatmaps/\" + \"horizontal_\" + courses[0] + \"- \" + method + \".png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294bcb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_sets = [\n",
    "    ('LIME', lime_normalized),\n",
    "    ('KernelSHAP', kernel_shap_normalized),\n",
    "    ('PermSHAP', perm_shap_normalized),\n",
    "    ('CEM', cem_normalized),\n",
    "    ('DiCE', counter_normalized)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6407b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5 = [f[:5].index.values for f in feature_sums]\n",
    "top_5 = [item for sublist in top_5 for item in sublist]\n",
    "set(top_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e0882c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "exp_sets = [\n",
    "    ('LIME', lime_normalized),\n",
    "    ('KernelSHAP', kernel_shap_normalized),\n",
    "    ('PermSHAP', perm_shap_normalized),\n",
    "    ('CEM', cem_normalized),\n",
    "    ('DiCE', counter_normalized)\n",
    "]\n",
    "feature_sums = []\n",
    "for method, experiment_set in exp_sets:\n",
    "    print(method)\n",
    "    fig, ax = plt.subplots(figsize=(10, 5),facecolor='white')\n",
    "#     for i in range(len(courses)):\n",
    "    i = 0\n",
    "    df = experiment_set[i][1]\n",
    "    if len(df) > 1:\n",
    "        if method == 'DiCE':\n",
    "            df = df.drop('exp number',axis=1).drop('exp_num',axis=1).drop('Unnamed: 0',axis=1).mean()\n",
    "        else:\n",
    "            if method == 'LIME':\n",
    "                df = df.drop('real value',axis=1)\n",
    "            df = df.drop('exp number',axis=1).mean() #.drop('exp_num',axis=1).drop('Unnamed: 0',axis=1)\n",
    "\n",
    "        df = pd.DataFrame(df)\n",
    "        df.index = [max(i.split(' '), key=len) for i in df.index]\n",
    "        df = df[~df.index.duplicated()]\n",
    "        if method == 'LIME':\n",
    "            df['week'] = [int(i.split('_InWeek')[1].split(' ')[0]) for i in df.index]\n",
    "            df['feature_name'] = [i.split('_InWeek')[0] for i in df.index]\n",
    "        else:\n",
    "            df['week'] = [int(i.split('_InWeek')[1]) for i in df.index]\n",
    "            df['feature_name'] = [i.split('_InWeek')[0].split(' ')[-1] for i in df.index]\n",
    "            if method == 'CEM' or method == 'DiCE':\n",
    "                df['feature_name'] = [mapping(i.split('_InWeek')[0].split(' ')[-1]) for i in df.index]\n",
    "        heatmap_df = df.pivot(\"feature_name\",\"week\", 0)\n",
    "#         print(heatmap_df)\n",
    "        if method == 'LIME':\n",
    "            for j in np.arange(10, 0, -1):\n",
    "                if j not in heatmap_df.columns:\n",
    "                    new_col = np.empty((len(heatmap_df),1))\n",
    "                    new_col.fill(0)\n",
    "                    heatmap_df.insert(0, j, new_col)\n",
    "            for col in index:\n",
    "                if col not in heatmap_df.index:\n",
    "                    row_nan = np.empty((10))\n",
    "                    row_nan.fill(0)\n",
    "                    new_row = pd.Series(data=row_nan, name=col)\n",
    "                    heatmap_df = heatmap_df.append(new_row, ignore_index=False)\n",
    "            heatmap_df = heatmap_df.drop(0, axis=1)\n",
    "        heatmap_df = heatmap_df.fillna(0)\n",
    "        numeric_df = heatmap_df.apply(lambda x: np.log10(x) if np.issubdtype(x.dtype, np.number) else x)\n",
    "#         feature_sums.append(heatmap_df.sum(axis=1).sort_values(ascending=False))\n",
    "        numeric_df = numeric_df[numeric_df.index.isin(set(top_5))]\n",
    "# #         print(heatmap_df)\n",
    "        ax = sns.heatmap(numeric_df, vmax=1, vmin=-4, cmap='magma_r', square=True)\n",
    "#         ax = sns.heatmap(heatmap_df, robust=True)\n",
    "        ax.set_title(courses[i] + \"- \" + method)\n",
    "        plt.savefig(\"Heatmaps/\" + \"expanded_\" + courses[i] + \"- \" + method + \".svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b30189",
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_normalized[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0175f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_normalized[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace10ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "exp_sets = [\n",
    "    ('LIME', lime_normalized),\n",
    "    ('KernelSHAP', kernel_shap_normalized),\n",
    "    ('PermSHAP', perm_shap_normalized),\n",
    "    ('CEM', cem_normalized),\n",
    "    ('DiCE', counter_normalized)\n",
    "]\n",
    "courses = ['dsp_001', 'dsp_002', 'geomatique_003', 'villesafricaines_001', 'microcontroleurs_003']\n",
    "courses_len = [10, 10, 15, 13, 13]\n",
    "all_points = {}\n",
    "for i in np.arange(5):\n",
    "    for method, experiment_set in exp_sets:\n",
    "        print(method)\n",
    "        print(courses[i])\n",
    "    #     fig, ax = plt.subplots(figsize=(10, 5),facecolor='white')\n",
    "    #     for i in range(len(courses)):\n",
    "        df = experiment_set[i][1]\n",
    "        if len(df) > 1:\n",
    "            if method == 'DiCE':\n",
    "                df = df.drop('exp number',axis=1).drop('exp_num',axis=1).drop('Unnamed: 0',axis=1)\n",
    "            else:\n",
    "                if method == 'LIME':\n",
    "                    df = df.drop('real value',axis=1)\n",
    "                df = df.drop('exp number',axis=1) #.drop('exp_num',axis=1).drop('Unnamed: 0',axis=1)\n",
    "\n",
    "            df = pd.DataFrame(df)\n",
    "            df = df.T\n",
    "            df.index = [max(i.split(' '), key=len) for i in df.index]\n",
    "            df = df[~df.index.duplicated()]\n",
    "            if method == 'LIME':\n",
    "                df.insert(0, 'week', [int(i.split('_InWeek')[1].split(' ')[0]) for i in df.index])\n",
    "                df['feature_name'] = [i.split('_InWeek')[0] for i in df.index]\n",
    "            else:\n",
    "                df.insert(0, 'week', [int(i.split('_InWeek')[1]) for i in df.index])\n",
    "                df['feature_name'] = [i.split('_InWeek')[0].split(' ')[-1] for i in df.index]\n",
    "                if method == 'CEM' or method == 'DiCE':\n",
    "                    df['feature_name'] = [mapping(i.split('_InWeek')[0].split(' ')[-1]) for i in df.index]\n",
    "            df.set_index(['feature_name', 'week'], drop=True, inplace=True)\n",
    "#             for j in np.arange(1, courses_len[i]+1):\n",
    "#             for j in np.arange(1, courses_len[i]+1):\n",
    "#                 if j not in df.columns:\n",
    "#                     new_col = np.empty((len(df),1))\n",
    "#                     new_col.fill(0)\n",
    "#                     df.insert(int(j)-1, j, new_col)\n",
    "            for col in feature_names:\n",
    "                for week in np.arange(1, courses_len[i]+1):\n",
    "                    if (col, week) not in df.index:\n",
    "                        row_nan = np.empty(len(df.columns))\n",
    "                        row_nan.fill(0)\n",
    "                        new_row = pd.Series(data=row_nan, name=(col, week))\n",
    "                        df = df.append(new_row, ignore_index=False)\n",
    "            df = df.fillna(0)\n",
    "            df = df.sort_index()\n",
    "    #         numeric_df = heatmap_df.apply(lambda x: np.log10(x) if np.issubdtype(x.dtype, np.number) else x)\n",
    "            all_points[(courses[i], method)] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729162d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points[('dsp_002', 'PermSHAP')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5958d23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "exp_sets = [\n",
    "    ('LIME', lime_normalized),\n",
    "    ('KernelSHAP', kernel_shap_normalized),\n",
    "    ('PermSHAP', perm_shap_normalized),\n",
    "    ('CEM', cem_normalized),\n",
    "    ('DiCE', counter_normalized)\n",
    "]\n",
    "courses = ['dsp_001', 'dsp_002', 'geomatique_003', 'villesafricaines_001', 'microcontroleurs_003']\n",
    "courses_len = [10, 10, 15,13,13]\n",
    "# heatmaps = {}\n",
    "for i in np.arange(5):\n",
    "    for method, experiment_set in exp_sets:\n",
    "        print(method)\n",
    "        print(courses[i])\n",
    "    #     fig, ax = plt.subplots(figsize=(10, 5),facecolor='white')\n",
    "    #     for i in range(len(courses)):\n",
    "        df = experiment_set[i][1]\n",
    "        if len(df) > 1:\n",
    "            if method == 'DiCE':\n",
    "                df = df.drop('exp number',axis=1).drop('exp_num',axis=1).drop('Unnamed: 0',axis=1).mean()\n",
    "            else:\n",
    "                if method == 'LIME':\n",
    "                    df = df.drop('real value',axis=1)\n",
    "                df = df.drop('exp number',axis=1).mean() #.drop('exp_num',axis=1).drop('Unnamed: 0',axis=1)\n",
    "\n",
    "            df = pd.DataFrame(df)\n",
    "            df.index = [max(i.split(' '), key=len) for i in df.index]\n",
    "            df = df[~df.index.duplicated()]\n",
    "            if method == 'LIME':\n",
    "                df['week'] = [int(i.split('_InWeek')[1].split(' ')[0]) for i in df.index]\n",
    "                df['feature_name'] = [i.split('_InWeek')[0] for i in df.index]\n",
    "            else:\n",
    "                df['week'] = [int(i.split('_InWeek')[1]) for i in df.index]\n",
    "                df['feature_name'] = [i.split('_InWeek')[0].split(' ')[-1] for i in df.index]\n",
    "                if method == 'CEM' or method == 'DiCE':\n",
    "                    df['feature_name'] = [mapping(i.split('_InWeek')[0].split(' ')[-1]) for i in df.index]\n",
    "            print(df)\n",
    "            heatmap_df = df.pivot(\"feature_name\",\"week\", 0)\n",
    "#             print(heatmap_df)\n",
    "            for j in np.arange(1, courses_len[i]+1):\n",
    "                if j not in heatmap_df.columns:\n",
    "                    new_col = np.empty((len(heatmap_df),1))\n",
    "                    new_col.fill(0)\n",
    "                    heatmap_df.insert(int(j)-1, j, new_col)\n",
    "            for col in master_list:\n",
    "                if col not in heatmap_df.index:\n",
    "                    row_nan = np.empty((courses_len[i]))\n",
    "                    row_nan.fill(0)\n",
    "                    new_row = pd.Series(data=row_nan, name=col)\n",
    "                    heatmap_df = heatmap_df.append(new_row, ignore_index=False)\n",
    "            if 0 in heatmap_df.columns:\n",
    "                heatmap_df = heatmap_df.drop(0, axis=1)\n",
    "            heatmap_df = heatmap_df.fillna(0)\n",
    "    #         numeric_df = heatmap_df.apply(lambda x: np.log10(x) if np.issubdtype(x.dtype, np.number) else x)\n",
    "            heatmaps[(courses[i], method)] = heatmap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list = [item for sublist in master_index for item in sublist]\n",
    "master_list = [item for sublist in master_list for item in sublist]\n",
    "set(master_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0672f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmaps[('microcontroleurs_003', 'PermSHAP')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21faef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = heatmap_df.index\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7945e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(feature):\n",
    "    name_mapping = {\n",
    "        'competency_anticipation':  'CompetencyAnticipation',\n",
    "         'content_alignment':  'ContentAlignment',\n",
    "         'content_anticipation':  'ContentAnticipation',\n",
    "         'delay_lecture':  'DelayLecture',\n",
    "         'frequency_action_Video.Load':  'FrequencyEventLoad',\n",
    "         'frequency_action_Video':  'FrequencyEventVideo',\n",
    "         'frequency_action_Video.Play':  'FrequencyEventVideoPlay',\n",
    "         'frequency_action_Video.Pause':  'FrequencyEventVideoPause',\n",
    "         'number_sessions':  'NumberOfSessions',\n",
    "         'ratio_clicks_weekend_day':  'RatioClicksWeekendDay',\n",
    "         'regularity_peak_dayhour':  'RegPeakTimeDayHour',\n",
    "         'regularity_periodicity_m1':  'RegPeriodicityM1',\n",
    "         'student_speed':  'StudentSpeed',\n",
    "         'time_between_sessions_std':  'StdTimeBetweenSessions',\n",
    "         'time_in__problem_sum':  'TotalTimeProblem',\n",
    "         'time_in__video_sum':  'TotalTimeVideo',\n",
    "         'time_sessions_mean':  'AvgTimeSessions',\n",
    "         'time_sessions_std':  'StdTimeSessions',\n",
    "         'time_sessions_sum':  'TotalTimeSessions',\n",
    "         'total_clicks':  'TotalClicks',\n",
    "         'total_clicks_Video':  'TotalClicksVideoChen',\n",
    "         'total_clicks_Video.Load':  'TotalClicksVideoLoad',\n",
    "         'total_clicks_problem':  'TotalClicksProblem',\n",
    "         'total_clicks_video':  'TotalClicksVideoConati',\n",
    "         'total_clicks_weekday':  'TotalClicksWeekday',\n",
    "         'total_clicks_weekend': 'TotalClicksWeekend',\n",
    "         'weekly_prop_replayed_mean': 'AvgReplayedWeeklyProp',\n",
    "         'weekly_prop_watched_mean':  'AvgWatchedWeeklyProp',\n",
    "         'weekly_prop_interrupted_mean': 'AvgInterruptedWeeklyProp',\n",
    "         'pause_duration_mean': 'AvgPauseDuration',\n",
    "         'pause_duration_std': 'StdPauseDuration',\n",
    "         'time_speeding_up_mean': 'AvgTimeSpeedingUp',\n",
    "         'time_speeding_up_std': 'StdTimeSpeedingUp'\n",
    "    }\n",
    "    \n",
    "    if feature in name_mapping.keys(): \n",
    "        return name_mapping[feature]\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bbd6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84deb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(['AvgReplayedWeeklyProp',\n",
    " 'CompetencyAnticipation',\n",
    " 'DelayLecture',\n",
    " 'FrequencyEventLoad',\n",
    " 'NumberOfSessions',\n",
    " 'RegPeakTimeDayHour',\n",
    " 'RegPeriodicityM1',\n",
    " 'TotalClicksProblem',\n",
    " 'TotalTimeProblem',\n",
    "'AvgReplayedWeeklyProp',\n",
    " 'AvgTimeSessions',\n",
    " 'CompetencyAlignment',\n",
    " 'CompetencyAnticipation',\n",
    " 'CompetencyStrength',\n",
    " 'ContentAlignment',\n",
    " 'ContentAnticipation',\n",
    " 'DelayLecture',\n",
    " 'RatioClicksWeekendDay',\n",
    " 'RegPeakTimeDayHour',\n",
    " 'RegPeriodicityM1',\n",
    " 'StdTimeSessions',\n",
    " 'StudentShape',\n",
    " 'StudentSpeed',\n",
    " 'TotalTimeProblem',\n",
    " 'TotalTimeVideo']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5abac85",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c3631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine, jensenshannon\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def extract(m1, m2, i):\n",
    "    x = m1[i]\n",
    "    y = m2[i]\n",
    "    if sum(x) == 0:\n",
    "        x[0] = 0.0001\n",
    "    return x,y\n",
    "\n",
    "# cosine distance\n",
    "def cosine_vector(m1, m2):\n",
    "  \"\"\"\n",
    "  in:\n",
    "  m1, m2: DataFrames of shape (n_instances, n_features)\n",
    "          containing feature importance scores from method 1/2\n",
    "  \n",
    "  out:\n",
    "  list of length n_instances, containing cosine distances\n",
    "  between feature importance scores for each instance\n",
    "  \"\"\"\n",
    "  dists = []\n",
    "  for i in range(min(len(m1.columns), len(m2.columns))):\n",
    "    x,y = extract(m1, m2, i)\n",
    "    dists.append(cosine(x,y))\n",
    "  return dists\n",
    "\n",
    "# norm of differences\n",
    "def norm_differences(m1, m2):\n",
    "  \"\"\"\n",
    "  in:\n",
    "  m1, m2: DataFrames of shape (n_instances, n_features)\n",
    "          containing feature importance scores from method 1/2\n",
    "  \n",
    "  out:\n",
    "  Frobenius norm of the difference of the two dataframes\n",
    "  \"\"\"\n",
    "  return np.linalg.norm(m1-m2, ord='fro')\n",
    "\n",
    "# jensen_shannon distance\n",
    "def jensen_shannon(m1, m2):\n",
    "  \"\"\"\n",
    "  in:\n",
    "  m1, m2: DataFrames of shape (n_instances, n_features)\n",
    "          containing feature importance scores from method 1/2\n",
    "  \n",
    "  out:\n",
    "  list of length n_instances, containing jenson shannon distance\n",
    "  between feature importance scores for each instance\n",
    "  \"\"\"\n",
    "  dists = []\n",
    "  for i in range(min(len(m1.columns), len(m2.columns))):\n",
    "    x, y = extract(m1, m2, i)\n",
    "    dists.append(jensenshannon(x,y))\n",
    "  dists = np.array(dists)\n",
    "  return dists\n",
    "\n",
    "# rank-correlation (spearman's rank correlation coefficient) \n",
    "def spearman_rank_correlation(m1, m2):\n",
    "  \"\"\"\n",
    "  in:\n",
    "  m1, m2: DataFrames of shape (n_instances, n_features)\n",
    "          containing feature importance scores from method 1/2\n",
    "  \n",
    "  out:\n",
    "  list of length n_instances, containing rank correlation\n",
    "  between feature importance scores for each instance\n",
    "  \"\"\"\n",
    "  dists = []\n",
    "  for i in range(min(len(m1.columns), len(m2.columns))):\n",
    "    x,y = extract(m1, m2, i)\n",
    "    corr, pval = spearmanr(x,y)\n",
    "    if corr is np.nan:\n",
    "        print(x, y)\n",
    "        print(sum(x), sum(y), corr)\n",
    "    dists.append(corr)\n",
    "  dists = np.array(dists)\n",
    "  return dists\n",
    "\n",
    "from sklearn.metrics import mutual_info_score\n",
    "# mutual information \n",
    "def mutual_info(m1, m2):\n",
    "  \"\"\"\n",
    "  in:\n",
    "  m1, m2: DataFrames of shape (n_instances, n_features)\n",
    "          containing feature importance scores from method 1/2\n",
    "  \n",
    "  out:\n",
    "  list of length n_instances, containing mutual info\n",
    "  between feature importance scores for each instance\n",
    "  \"\"\"\n",
    "  dists = []\n",
    "  for i in range(min(len(m1.columns), len(m2.columns))):\n",
    "    x,y = extract(m1, m2, i)\n",
    "    dists.append(mutual_info_score(x,y))\n",
    "  dists = np.array(dists)\n",
    "  return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe7f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(all_points[('dsp_002', 'CEM')].index) - set(all_points[('dsp_002', 'LIME')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d39179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "method_list = ['LIME', 'PermSHAP', 'KernelSHAP', 'DiCE', 'CEM']\n",
    "all_results = []\n",
    "course_names = ['dsp_001', 'dsp_002', 'geomatique_003', 'villesafricaines_001', 'microcontroleurs_003']\n",
    "distance_metric = mutual_info\n",
    "for course in course_names:\n",
    "    comparison_results = []\n",
    "    for method_i in method_list:\n",
    "        for method_j in method_list:\n",
    "            print(course, method_i, method_j)\n",
    "            comparison_results.append(distance_metric(all_points[(course, method_i)], all_points[(course, method_j)]))\n",
    "    all_results.append(comparison_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848015cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19102145",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_dict = {}\n",
    "method = 'mutual_info'\n",
    "pretty_name = \"Mutual Info\"\n",
    "# course_index = 0\n",
    "for course_index in np.arange(5):\n",
    "    plt.figure()\n",
    "    for i, course in enumerate(all_results):\n",
    "        comp_mean = []\n",
    "        for comparison in course:\n",
    "            comp_mean.append(np.mean(comparison))\n",
    "        dist_dict[i] = comp_mean\n",
    "    heatmap_df = pd.DataFrame(np.array(dist_dict[course_index]).reshape(5,5), columns=['LIME', 'KernelSHAP', 'PermSHAP',  'DiCE', 'CEM'])\n",
    "    heatmap_df['method'] = ['LIME', 'KernelSHAP', 'PermSHAP',  'DiCE', 'CEM']\n",
    "    heatmap_df = heatmap_df.set_index('method')\n",
    "    sns.heatmap(heatmap_df, cmap='magma', annot=True, vmin=0, vmax=1, fmt=\"0.2f\", annot_kws={\"fontsize\":15})\n",
    "    plt.title(pretty_name + ' between \\nexplainability methods in '+courses[course_index])\n",
    "    plt.savefig(\"plots/\" + courses[course_index] + \"_\" + method + \".png\", bbox_inches='tight')\n",
    "    \n",
    "data = json.dumps(dist_dict)\n",
    "# open file for writing, \"w\" \n",
    "f = open(method +\".json\",\"w\")\n",
    "f.write(data)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
